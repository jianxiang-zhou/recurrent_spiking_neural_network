# recurrent_spiking_neural_network
A recurrent spiking neural network model trained with 3 factor rules to learn sensorimotor associations

The RSNN model is composed of 200 leaky integrate-and-fire (LIF) neurons, each having 20 input and 20 output excitatory synapses with other LIF neurons. The dynamics of neuron $i$ is modeled as\
$${dv_i (t) \over dt}={v_{rest,i}-v_i (t) \over τ}+\sum_j w_{ij}·s_j (t-t_{delay})$$\
where $v$ is the membrane potential, $v_{rest}$ is the resting membrane potential, $τ$ is the time constant of membrane potential, $w_{ij}$ is the synaptic weight from neuron $j$ to neuron $i$, $t_{delay}$ is the synaptic delay of spikes, and $s_j$ is the spike train of neuron $j$ modelled as\
$$s_j (t)= \sum_k δ(t-t_j^{(k)})$$\
where the $t^{(k)}$ is the time of $k_{th}& spike, $δ(·)$ is the Dirac delta function. When membrane potential reaches a threshold $v_{threshold}$, a spike is generated, and the membrane potential $v$ is reset to $v_{reset}$. To simulate the spontaneous activity of neurons, random currents are injected into the LIF neurons. All LIF neurons are excitatory, and the network activity is balanced by an inhibitory neuron group receiving inputs from and projecting back to all LIF neurons equally, mediating lateral inhibition between LIF neurons.
The RSNN interacts with the environment using defined sensory and motor neurons. Forty LIF neurons are selected as sensory neurons that are directly excited by stimuli (twenty for stimulus A and twenty for stimulus B), and two other LIF neurons are selected as motor neurons whose firings determine the actions of the neural network (i.e., left and right choices). A reward is given when RSNN makes left choice after stimulus A or right choice after stimulus B. The reward drives learning through three-factor learning rule (Frémaux and Gerstner 2016) modeled as\
(de_ij)/dt=-e_ij/τ_e +STDP(s_j,s_i)
(dw_ij)/dt=R·e_ij
where eij is the eligibility trace of synapse from neuron j to neuron i, τe is the time constant of the eligibility trace, sj and si are spike trains of neuron j and i, respectively, and STDP(·) is the function mapping pre- and post-synaptic spike trains to the change of the eligibility trace. The reward is represented as a global signal R which interacts with local eligibility traces e to cause permanent changes of synaptic weights w. In a nutshell, if some activity pattern was generated before the reward (e.g., through spontaneous activity), the three-factor learning rule would selectively strengthen the synapses supporting that activity pattern. Therefore, the behavior represented by that activity pattern would appear more frequently in the future, helping the animal to get more rewards.
The RSNNs were trained for 2000 trials. The spike trains of LIF neurons were analyzed after training. In this network, neuronal activity was mainly driven by stimulus. We quantified the stimulus selectivity of neurons using receiver operator characteristic (ROC) analysis, and defined selectivity index (SI) of neurons as the 2*|AUC-0.5| (AUC, area under ROC curve). Neurons with SI larger than 0.5 were considered as task related.
